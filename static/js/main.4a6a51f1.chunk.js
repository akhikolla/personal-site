(this["webpackJsonppersonal-site"]=this["webpackJsonppersonal-site"]||[]).push([[0],{154:function(e,n,t){"use strict";t.r(n);var a=t(1),s=t.n(a),i=t(35),o=t(7),r=t(3),l=t(16),d=(t(80),t(0));var c=t(39),h=t.n(c);const u=[{title:"Numbers To Know",id:"numbers-to-know",markdown:"## Modern Hardware Limits\n\nAWS  M6i.32xlarge - 512 GB memory and 128 vCPUs for general workloads. \n\n### Memory Optimized instances: \n\nX1e.32xlarge provides 4 TB of RAM, while the U-24tb1.metal reaches 24 TB of RAM.\n\n## Caching: \n\nToday's caches routinely handle terabyte-scale datasets with single digit millisecond latency. \n\n* Memory: Upto 1TB on memory optimized instances. \n* Latency: \n    Read: < 1ms within same region\n    Write: 1-2 ms average cross region for optimized systems. \n* Throughput: \n    Reads: Over 100k requests/second per instance for in-memory caches like ElasticCache Redis on modern graviton based nodes. \n    Writes: Sustained throughput of hundreds of thousands of requests per second. \n\n### When to consider Sharding: \n\n1. When dataset size is approaching 1TB. \n2. Sustained throughput of 100k+ ops/second\n3. Requirements below 0.5 ms consistently. \n\n\n## Databases: \n\nSingle PostgreSQL or MySQL instances can routinely handle dozens of terabytes of data while maintaining a milli second response times. \n\n* Storage: Single instance can handle upto 64TiB for most of the database engines, with aurora supporting upto 128TB in some configurations. \n* Latency: \n    * Read:  1-5 ms for cached data, 5-30 ms for disk(optimized configurations for RDS and Aurora)\n    * Write: 5-15ms for commit latency (for single node, high performance setups)\n* Throughput: \n    Reads: upto 50k TPS in single node configurations on Aurora and RDS. \n    Writes: 10-20k TPS in single node configurations on Aurora and RDS. \n* Connections: 5-20k concurrent connections based on database and instance type. \n\n### When to consider Sharding: \n\n1. Database size approaching or exceeding 50TB. \n2. Write throughput conssitently exceeding 10k TPS. \n3. Read Latency exceeding 5ms for uncached data \n4. Geographic distribution, cross region replication or distributed needs. \n5. Backup windows that stretch into the hours or become operationally impractical. \n\n## Application Servers: \n\n* Memory: 64-512 GB standard, upto 2TB on high memory instances. \n* CPU: 8-64 cores. \n* Connections: 100k+ concurrent connections per instance for optimized configuration. \n* Network: Upto 25 Gbps bandwidth in modern server configuration. \n* Startup Time: 30-60 seconds for containerized apps. \n\n### When to consider Sharding: \n\n1. CPU utilization consistently above 70-80%\n2. Response Latency exceeding the SLA or critical thresholds. \n3. Memory usage trending above 70-80%\n4. Network bandwidth approaching 20Gbps. \n\n## Message Queues: \n\nMessage queues have transformed simple task delagations systems into high performance data highways. Systems like kafka process millions of messages per second with single digit millisecond latency, while maintaining weeks or months of data. \n\n* Storage: Upto 50TB per broker in advanced configurations.\n* Latency: 1-5ms end2end within a region for optimized setups. \n* Throughput: Upto 1 million messages/second per broker in modern configurations. \n* Message Size: 1KB-10MB efficiently handled. \n* Retention: Weeks to months of data, depending on disk capacity and configuration. \n\n### When to consider Sharding: \n\n1. Throughput: Nearing 800k messages/second per broker. \n2. Partition Count: Approaching 200k per cluster.\n3. Consumer Lag: Consistently growing, impacting real time processing. \n4. Cross Region Replication: If geographic redundancy is required. \n\n\n### Common Mistakes: \n\n1. Premature sharding: \n\nSharding is not always necessary. \n\nDesign Yelp: \n10M businesses, each of which is roughly 1KB of data = 10 M * 1KB = 10GB of data! 10x of this 10GB to account for reviews, which can store in the same database and you're only at 100GB. why would you shard? \n\nDesign Leetcode: \nSame comes up with caches, we have 100K competitions and up to 100K users per competition, we are looking at 100K * 100K * 36b ID + 4b float rating = 400GB, which is more than what we store at disk, this can still fit on a single large cache - no need to shard. \n\n2. Overestimating latency: \n\nI see this most with SSDs. Candidates tend to vastly overestimate the latency additonal to query an SSD(database) for a simple key or row lookup. \n \nWe're talking 10 ms or so. Oftentimes we justify this by adding a cache layer to reduce latency when the simple row lookup is already fast enough - no need for additional infrastructure. \n\nHowever, we need to note that this is applicable only for simple row look ups with an index. It's still wise to cache expensive queries. \n\n3. Over engineering given a high write throughput: \n\nImagine we have system with 5K writes per second, Candidates will often jump to adding a message queue to buffer this high write throughput, but we don't need to. \n\nLet's put this in perspective. A well tuned postgres instance with simple writes can handle upto 20k+ writes per second. \n\n#### What actually limits write capacity are: \n\n1. Complex transactions spanning multiple tables. \n2. Write amplification from excessive indexes. \n3. Writes that trigger expensive cascading updates. \n4. Heavy concurrent reads competing with writes. \n\n* Message queues become valuable when you need guaranteed delivery in case of downstream failures, event sourcing patterns, handling write spikes above 50K+ WPS, or decoupling producers from consumers. \n* Before reaching for message queues, consider simpler optimizations like batch writes, optimizing your schema and indexes, using connection pooling effectively, or using async  commits for non critical writes. \n\n\n"},{title:"Scaling Reads",id:"scaling-reads",markdown:"There is an imbalance in terms of the reads and writes that a system could contain.\n\nFor example,\nevery tweet that's posted, thousands of users read it.\nevery product that's uploaded to amazon, hundreds browse it.\nyoutube processes billions of video views daily but only millions of uploads.\n\nThe standard read to write ratio starts at 10:1 but often reachs 100:1 or higher for content heavy applications. As no of reads increase database will struggle to load.\n\nThis is a physics problem:\n\n1. CPU cores can only execute so many instructions per second.\n2. Memory can hold only so much data\n3. Disk I/O is bounded by speed of spinning platters or SSD write cycles.\n\n## Solution: Simple optimizations to complex distributed systems.\n\n1. Optimize read performance within your database.\n2. Scale your database horizontally.\n3. Add external caching layers.\n\n### Optimize within the Database\n\n1. Indexing: \n\n* Similar to book index. Instead of scanning every page to find mentions of the database, you check the index at the back which tells you exactly which pages to look at.\n* When queried without an index, the database performs what's called a full table scan, it reads every single row to find matches. With an index, it can jump directly to relevant rows.\n* Turns O(n) operation to O(log n), which is differences between scanning 1 million rows versus checking may be 20 index entries.\n\nDifferent types of indexes: \n\n1. B-Tree for general queries. \n2. Hash Indexes work well for exact matches\n3. Specialized indexes handle full text search or geographical queries. \n\n* Addressing Read scaling problem is to add indexes to columns we frequently query, join on or sort by. \n* If designing a social media app and users often search for posts by hashtag, index the hashtag column, If sorting products by price, index price column. \n* In interviews, confidently add indexes for your query patterns - under-indexing kills more applications than over-indexing ever will.\n\n2. Hardware upgrades: \n\n* Use better and bigger hardware. SSD provides 10-100x faster random I/o than spinning disks. More RAM means more data stays in memory, avoiding disk reads entirely. Faster CPUs and more cores handle more concurrent queries.\n\n3. Denormalization Strategies: \n\n* Normalization is the process of structuring data to reduce redundancy by splitting information across multiple tables to aovid storing duplicate data. While it saves storage space it makes queries more complex because we need joins to bring related data back together. \n* For read-heavy systems, denormalization (the opposite of normalization where you store redundant data) trades storage for speed. Instead of joining three tables to get user profile data, store the data redundantly in a single table.\n\n\n### Scale database horizontally\n\n1. Read Replicas\n\n* The first approach to scaling beyond a single database is adding read replicas. Read replicas copy data from your primary database to additional servers. All writes go to the primary, but reads can go to any replica. This distributes read load across multiple servers.\n* Leader-follower replication is the standard approach. One primary (leader) handles writes, multiple secondaries (followers) handle reads. Replication can be synchronous (slower but consistent) or asynchronous (faster but potentially stale).\n* The key challenge is replication lag. When you write to the primary, it takes time to propagate to replicas. Users might not see their own changes immediately if they're reading from a lagging replica.\n\n2. Database Sharding\n\n* Read replicas distribute load but don't reduce dataset size that each database needs to handle. Sharding can help by splitting data across multiple databases.\n* For read scaling, sharding helps in two main ways: smaller datasets mean faster individual queries, and you can distribute read load across multiple databases.\n* Functional sharding splits data by business domain or feature rather than by records. Put user data in one database, product data in another. Now user profile requests only query the smaller user database, and product searches only hit the product database.\n* Geographic sharding is particularly effective for global read scaling. Store US user data in US databases, European data in European databases. Users get faster reads from nearby servers while reducing load on any single database.\n\n### Add External Caching Layers\n\n* Importantly, most applications exhibit highly skewed access patterns. On Twitter, millions read the same viral tweets. On e-commerce sites, thousands view the same popular products. This means you're repeatedly querying your database for identical data - data that rarely changes between requests.\n* Caches exploit this pattern by storing frequently accessed data in memory.\n\n1. Application-Level Caching\n\n* In-memory caches like Redis or Memcached sit between your application and database. When your application needs data, it checks the cache first. On a hit, you get sub-millisecond response times. On a miss, you query the database and populate the cache for future requests.\n* Cache invalidation remains the primary challenge. When data changes, you need to ensure caches don't serve stale data. Common strategies include:\n    * TTL time based expiration - set fixed time for cached entires. \n    * Write through invalidation - update/delete cached entires when writing to db. \n    * Write behind invalidation - queue invalidation events to process asynsly. \n    * Tagged invalidation - Assocaite cache entries with tags, invaliadte all entries with a specific tag when related data changes. \n    * Versioned keys - includes version numbers in cache keys. \n\n2. CDN and Edge Caching \n\n* Content Delivery Networks extend caching beyond your data center to global edge locations. While originally designed for static assets, modern CDNs cache dynamic content including API responses and database query results.\n"},{title:"Scaling Writes",id:"scaling-writes",markdown:"Write scaling isn't (only) about throwing more hardware at the problem, there's a bunch of architectural choices we can make which improve the system's ability to scale.\n\n* Vertical Scaling and Database Choices\n* Sharding and Partitioning\n* Handling Bursts with Queues and Load Shedding\n* Batching and Hierarchical Aggregation\n\n## Vertical Scaling and Write Optimization\n\n* We'll start with the hardware, or \"vertical scaling\". Writes are bottlenecked by disk I/O, CPU, or network bandwidth. We should confirm we're hitting those walls before we proceed. Often this means we need to do some brief back-of-the-envelope math to see both \n(a) what our write throughput actually is, and \n(b) whether that fits within our hardware capabilities.\n* A great example of this is using a write-heavy database like Cassandra. Cassandra achieves superior write throughput through its append-only commit log architecture. Instead of updating data in place (which requires expensive disk seeks), Cassandra writes everything sequentially to disk. This lets it handle 10,000+ writes per second on modest hardware, compared to maybe 1,000 writes per second for a traditional relational database doing the same work.\n* Cassandra's read performance isn't great. Reading data often requires checking multiple files and merging results, which can be slower than a well-indexed relational database.\n\n## Sharding and Partitioning\n\n### Horizontal Sharding\n\n* A great, simple example of sharding is what Redis Cluster does. Each entry in Redis is stored with a single string key. These keys are hashed using a simple CRC function to determine a \"slot number\". These slot numbers are then assigned to the different nodes in the cluster.\n* Clients query the Redis Cluster to keep track of servers in the cluster and the slot numbers they are responsible for. When a client wants to write a value, it hashes the key to determine the slot number, looks up the server responsible for that slot, and sends the write request to that server.\n\n### Vertical Partitioning\n\n* While horizontal sharding splits rows, vertical partitioning splits columns. You separate different types of data that have different access patterns and scaling requirements. Instead of cramming everything into one massive table, you break it apart based on how the data is actually used.\n\n## Handling Bursts with Queues and Load Shedding\n\n* While partitioning and sharding will get you 80% of the way to scale, they often stumble in production. Real-world write traffic isn't steady, and while scale often does smooth (Amazon's ordering volume is surprisingly stable), some bursts are common. Interviewers love to drill in on things like \"what happens on black friday, when order volume 4x's\" or \"during new years, we triple the number of drivers on the road\".\n\nwe either need to \n(a) buffer the writes so we can process them as quickly as we can without failure, or \n(b) get rid of writes in a way that is acceptable to the business. "},{title:"Video Catalog System",id:"video-catalog",markdown:"This service powers metadata for movies, shows, episodes, and makes it searchable, browsable, and filterable for end users across devices.\n\n## \ud83e\udde0 Clarifying Questions to Ask\n\nQ1: What types of content will we support \u2014 movies, shows, episodes, trailers, etc.?\nA: All of them. The catalog should handle series with multiple seasons, trailers, feature films, and upcoming content.\n\nQ2: Is the catalog global? Should it handle different languages, regional availability, and content ratings per country?\nA: Yes, the system must support global metadata with localization.\n\nQ3: What kind of users or services will consume this catalog \u2014 internal tools, recommendation engine, mobile/web clients?\nA: All of the above. The system must support high read throughput with low latency.\n\nQ4: Do we need to handle real-time updates? For example, can metadata change frequently (e.g., when dubbing gets added)?\nA: Updates are not in real-time but can be frequent \u2014 for example, daily ingest jobs or manual editor updates.\n\nQ5: What kind of metadata are we talking about? Just title and description, or rich data like cast, genre, maturity ratings, thumbnails?\nA: All of them. We want a complete metadata system that is extensible.\n\n## \u2705 Functional Requirements\n\n- Store and manage metadata for all types of content: movies, series, episodes, trailers.\n- Allow querying by title, genre, actors, tags, etc.\n- Handle hierarchical relationships (series \u2192 seasons \u2192 episodes).\n- Support localization (e.g., descriptions, subtitles in multiple languages).\n- Support regional restrictions (e.g., title available in US but not India).\n- Allow updates to metadata from content ingestion pipelines or internal editors.\n- Serve fast and efficient APIs to downstream services like search, homepage, recommendation engine.\n\n## \ud83d\ude80 Non-Functional Requirements\n\n- High Availability: The catalog should be accessible 24/7 globally.\n- Scalability: Must support millions of titles and billions of reads per day.\n- Low Latency: Responses under 100ms for user-facing calls.\n- Consistency: Metadata updates should eventually reflect across all services.\n- Durability: Metadata should never be lost.\n- Extensibility: Support for new metadata fields in the future.\n\n## \ud83d\udccc Assumptions\n\n- Reads >> Writes (high read throughput, low write volume).\n- Metadata size is relatively small (compared to video files).\n- Most queries are read-heavy, cacheable, and repeated frequently.\n- Metadata can be updated multiple times a day.\n- Multiple services (UI, search, recommendations) consume the same APIs.\n\n## \ud83d\udcd0 High-Level System Design\n\nAt a high level, we break the system into:\n\nMetadata Ingestion Layer\nIngests metadata from studios, content providers, and manual editors. Validates, transforms, stores data in DynamoDB.\n\nMetadata Store (Master DB)\nStores the full raw metadata records in a durable and normalized form. Partitioned by contentID\n\nSearch & Serving Layer (Read-Optimized)\nHandles queries using denormalized, indexed data. Optimized for queries, filters, autocomplete.\n\nAPI Layer\nExposes REST/gRPC endpoints to internal and external consumers.\n\nCache Layer\nReduces pressure on the serving layer. CDN Caches metadata blobs globally. Redis used for hot content(trending, homepage)\n\nLocalization & Regional Rules Engine\nFilters or adjusts responses based on country, language, or maturity rules.\n\n## TradeOffs:\n\nMetadata Storage: DynamoDB vs PostgreSQL\n\n- Why not SQL (PostgreSQL)?\n\n* SQL is great for relationships and joins, but Netflix metadata isn\u2019t deeply relational.\n* Scaling globally with Postgres involves sharding and high operational overhead.\n\n- Why DynamoDB?\n\n* Global scale out of the box.\n* Schemaless \u2014 can easily evolve metadata structure.\n* Easy to query by partition key (e.g., contentId) or secondary indexes (e.g., title).\n* Write-heavy and read-heavy workloads supported.\n* Backed by AWS and Netflix infra standards.\n\n\u2192 Choice: DynamoDB for metadata storage with Global Secondary Indexes (GSIs) for filtering/search.\n\nSearch Layer: Elasticsearch vs Solr vs Custom\n\n- Why Elasticsearch (ES)?\n\n* Full-text search on titles, descriptions, tags.\n* Faceted search (genre + rating + year).\n* Optimized for autocomplete, fuzzy search.\n* Netflix already uses it in production for other search use cases.\n\n\u2192 Choice: Elasticsearch for user-facing title search and filters.\n\nCaching: Redis + CDN\n\nRedis: For server-to-server caching (e.g., metadata by content ID).\nCDN: For edge caching of common metadata blobs (e.g., homepage rows).\n\n\u2192 Choice: Redis for internal, CloudFront or Akamai CDN for global edge caching.\n\n## \ud83d\udd17 APIs\n\n    GET /catalog/{id} \u2013 Fetch full metadata for a title\n\n    GET /catalog/search?title=&genre=Action \u2013 Search by filters\n\n    POST /catalog \u2013 Add new title (studio/internal CMS)\n\n    PATCH /catalog/{id} \u2013 Update metadata\n\n    GET /catalog/homepage?region=IN \u2013 Get homepage content for region\n\n\ud83e\udde0 How Do We Scale?\n\n- Horizontally scale API layer (stateless)\n- DynamoDB handles millions of TPS with partition keys\n- Elasticsearch clusters handle full-text indexing/search\n- Redis scales using clustering and key partitioning\n- CloudFront (or similar CDN) caches content closest to users\n\n\ud83d\udd25 Why Redis (and not Kafka, Flink, etc.)?\n\nRedis is great for caching. Kafka and Flink are streaming systems used for processing and transporting data, not for low-latency lookup.\n\nRedis gives:\n\n- Microsecond-level response time (compared to 10\u201350ms in Elasticsearch or 50\u2013100ms in DynamoDB).\n- Built-in TTL to auto-expire stale cache entries.\n- Data structures like sorted sets, hash maps for lightweight logic.\n- Horizontal scalability via Redis Cluster.\n\n\ud83d\udeab Why not Kafka/Flink?\nKafka is ideal for data ingestion pipelines (e.g., pushing new content to the catalog), not for key-value lookup.\nFlink is used for real-time stream processing (e.g., counting, filtering, anomaly detection).\nRedis is not a replacement for these \u2014 it\u2019s complementary. But in this use case, we don\u2019t need to stream data in real time to clients \u2014 we need fast access to metadata.\n\n\ud83e\udde0 \u201cCan it scale to serve billions of reads per day?\u201d \u2705 Yes.\n\n- API layer is stateless \u2014 easily horizontally scaled.\n- DynamoDB auto-scales for write throughput.\n- Elasticsearch is sharded and replicated.\n- Redis Cluster supports high concurrency.\n\n\ud83e\udde0 \u201cIs latency under 100ms?\u201d \u2705 Yes.\n\nHot paths use Redis (~1\u20132ms).\nFallbacks to Elasticsearch (~50ms).\nEdge caching via CDN (sub-10ms globally).\n"},{title:"Game leaderboard Topk",id:"game-leader-board",markdown:"We\u2019ll ingest frequent score updates, maintain real-time global rankings per game in Redis sorted sets, and answer Top-K global in O(log N + K). For Top-K among a user\u2019s friends, we\u2019ll intersect the global leaderboard with that user\u2019s friend set using Redis 7 ZINTER (server-side, limited to K) or a pre-materialized, TTL\u2019d \u201cfriends leaderboard\u201d for large friend lists. Kafka (or Kinesis) guarantees durable, ordered ingestion; DynamoDB/Cassandra stores the authoritative last score per (player, game). The system targets p95 read < 50 ms, p95 write < 100\u2013150 ms end-to-end, and sub-second freshness.\n\n## Functional Requirements\n\n- Ingest frequent score updates (incremental or absolute) for players across game modes/regions.\n- Query Top-K Global (per mode/region).\n- Query Top-K Among My Friends (per mode/region).\n\n- Real-time feel: clients auto-refresh or receive push updates when Top-K changes.\n- Return each entry\u2019s rank, score, player profile, and tie handling.\n\n- Support pagination (next-K) and \u201cwhat\u2019s my rank?\u201d.\n- Multi-tenancy: multiple game modes/shards/regions.\n- Admin/anti-cheat hooks for invalidation and recompute.\n\n## Non Functional Requirements\n\n- Latency (p95): reads < 50 ms, writes E2E < 150 ms (API->visible).\n- Freshness: eventual, typically < 1 s from ingest to visible rank.\n- Availability: 99.9%+ read APIs, 99.9% ingest.\n- Consistency: eventual for leaderboard; read-your-write not guaranteed.\n- Throughput (example planning):\n  - Peak writes: 100k updates/s across all games.\n  - Peak reads: 200k reads/s, 60% global, 40% friends.\n- Durability: all updates persist to the log (Kafka) and the source-of-truth DB\n"},{title:"Top K Problems",id:"topk",markdown:"Design a service which allows us to query the topK most viewed videos in a given time period. The time periods are fixed and might be minute, hour, day, month, or all-time.\n\n<p> </p>\n\n## Functional Requirements (FR)\n\n- Query Top K Most Viewed Videos - Support querying top-K videos (up to 1000)\n- Accept a time period as input\n\n## Non-Functional Requirements (NFR)\n\n- Consistency - Staleness requirement: videos should appear in top-K within 60 seconds\n- Performance - Read latency should be between 10ms to 100ms\n- Scalability - Must handle massive reads and writes\n- Accuracy - Approximations like Bloom Filter, Count-Min Sketch, or PNN are not allowed\n\n## Core Entities\n\nView, Video, Window (min/hour/Day/All-Time)\n\n## API\n\nview comes in we can connect to a stream, api call/kafka topic? represents this?\n\nInput - {videoId}\n\nAPI to get views - GET /views/video?k={k}&window={window} -> [{videoId, views}] - sortedlist\n\n- no, need for video metadata and assume we have a downstream aggregater that can read from me and then augument this videoId with the video names or client can make separate calls to get info.\n\n## HLD\n\nOptimal sol from scalibility and work backwards to system that actual functions.\n\nTake working solution and make it optimal. before NFRs. Don't take all FR together.\n\nshould we solve all time first and then get the window split check?\n\n- We are talking about avaiability -- obvious solution is to replicate the service. If we replicate the service we are bound to enable some kind of the load balancer or an api gateway if we intend deal with or to enable the ratelimiting or authentication and stuff.\n\n- if service fails, LB takes it out and client still have availability.\n- this is a stateful service, if we want to bring back that failed service we have a problem. we should try to manage state within our service such that it doesn't spread across the entire design. TopK is stateful but we need to figure out how to deal it.\n\n- if we lost one instances, we boot it back up and re read those messages from kafka if kafka has retention enabled and mesages and then repopulate the counts. We have to start from scratch. If service is already at bottle neck in terms of views it's ingesting we will have hard time catching up. let's say out system is already 80% utilized on steady state and where it doesn't actually need to catch up and only 20% available to work through backlog. I functionally working with quarter of second it will take more time to catch up.\n\nMost services don't have bunch of excess capacity to rework those jobs, in this case we probably want to do is enable checkpointing.\n\n- write out our counts,heap and ids of last view that we processed that we have to some kind of blob S3/Azure storage. when our service starts up we read from checkpoint, we restore all of the state and then we resume reading from stream from that latest time point, we can do checkpoint in hourly, minutely basis etc but this helps with recover, scale. We need service to add new service we can catch up quickly and check points aren't far behind. For keep consistency.\n\n- How to handle write volumes from kafka topic, we are gonna have multiple shards. How are shards organized. We can assigned that video to one topK service.We can assign that one video to one topK service. Easy way is to take module of number of shards, and videoIds are distributed in semi random fashion. Challenge is we would have to aggregate them across read side.\n\n- TopK service will need to query from each shard and aggregate it. We are going to have top 1000 views from each heap and be able to to accept an arbitary, we can guarantee that global 1000 is gonna be from one of these heap, all we need to do is merge sorted lists and functionally we need to iterate over each item only once. This solution provides way to shards out right and produce a value.\n\n- View stream is partitioned in the same way counters were. How can we make it more elastic? If we need to able to add more shards, we need Zookeeper, where we can keep track of no of shards out there, along side the range of avaiable videos in each shard. If we add a new shards we might need to remove videos from one shard and have to redistribute them, we use checkpoints and retention on kafka streams to pull this off.\n\n- Asynchronous replication with eventual consistency and local reads provides the best balance for streaming systems with strict latency requirements. It allows each node to serve reads immediately without waiting for cross-node synchronization, while still maintaining data durability through asynchronous replication. This approach minimizes latency while ensuring high availability, which is crucial for real-time analytics where slight data inconsistencies are often acceptable in exchange for continuous operation.\n\n- While precomputation typically trades storage for query latency, it doesn't 'always' reduce latency. Precomputed results may become stale, require complex invalidation, or create hotspots. The effectiveness depends on query patterns, update frequency, and cache hit rates.\n\n- When scaling stateful stream processors horizontally, the main challenge is redistributing accumulated state (like counters, heaps, or time windows) across new nodes. Unlike stateless processors that can instantly utilize new capacity, stateful systems must migrate existing state data, which can be expensive and temporarily impact processing during rebalancing. This is why techniques like consistent hashing and incremental state migration are critical for stateful distributed systems.\n\n## Deep Dive\n\n- Of all time counts is solved, how do we handle time windows? a considerable amount of time is needed.\n\n### Approaches:\n\n1. Producing arbitarary aggregation based on time:\n"},{title:"Rate Limiter Design",id:"rate-limiter",markdown:"## Rate Limiter"},{title:"Kafka",id:"kafka",markdown:"## Kafka notes:\n\n* Apache Kafka is an open-source distributed event streaming platform that can be used as either a message queue or stream processing system, designed for high performance, scalability, and durability.\n* Topics are logical groupings of partitions used for publishing and subscribing to data. Partitions are the physical storage units that allow parallel processing, and they are distributed across multiple brokers (servers) in the cluster.\n* Kafka uses a partitioning algorithm that hashes the message key to assign the message to a specific partition. This ensures that messages with the same key always go to the same partition, preserving order at the partition level.\n* Kafka partitions are append-only, immutable logs where messages cannot be altered or deleted once written.Each partition functions as an append-only log file where messages are sequentially added to the end. This immutability is central to Kafka's performance, reliability, and simplifies replication and recovery processes.\n* Consumer groups ensure that each message is processed by exactly one consumer within the group, enabling load balancing and parallel processing while preventing duplicate message processing.\n* Kafka uses a leader-follower replication model where each partition has a leader replica handling reads/writes and multiple follower replicas that passively replicate data, ensuring durability and availability even if brokers fail. This is how it handles fault tolerance for message durability. \n* Kafka uses a pull-based model where consumers actively poll brokers for new messages. This design choice allows consumers to control their consumption rate, prevents overwhelming slow consumers, and enables efficient batching.\n* While message size can be configured, it's recommended to keep messages under 1MB to ensure optimal performance through reduced memory pressure and better network utilization.\n* A hot partition occurs when one partition receives much more traffic than others (e.g., a popular ad getting many clicks), potentially overwhelming that partition's broker while leaving other brokers underutilized.\n* Kafka cannot automatically redistribute hot partitions. Solutions include random partitioning, salting keys with random values, using compound keys, or implementing back pressure - but not automatic redistribution.\n* Setting acks=all ensures that a message is acknowledged only when all in-sync replicas have received it, providing maximum durability guarantees at the cost of slightly higher latency.\n* Consumers commit their offsets to Kafka after processing messages. When a consumer restarts, it reads its last committed offset and resumes from there, ensuring no messages are missed or duplicated.\n* Kafka is designed as a high-performance streaming platform rather than a traditional message queue. Consumer retry patterns must be implemented using separate retry topics and dead letter queues rather than built-in mechanisms.\n* Batching messages allows producers to send multiple messages in a single request, significantly reducing network overhead and improving throughput. This can be configured with maxSize and maxTime settings.\n* Kafka topics have configurable retention policies controlled by retention.ms and retention.bytes settings. The default is to retain messages for 7 days (168 hours) with retention.bytes set to -1 (no size limit). Size limits can be configured but are not set by default.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{title:"Real Time Updates",id:"real-time-updates",markdown:"# Examples that need real time updates\n\nGoogle docs work when multiple ppl are making edits, drivers get notifications ride is ready, new comments pop up during live stream.\n\n### Things to Remember:\n\n    1. Polling should be default option. It's stateless and work with many infras\n    2. Always ask interview how real time does this needs to be? pretty real time or fairly fast - using polling.\n\nTypical http follows a request response model, it follows a 3 way handshake and 4 way teardown, burning lot of network overhead for nothing, standard http falls short for real time. This isn't conducive to real time updates.\n\n## Options: Techniques to simulate a server initiated connection\n\n1. WebSockets: Bidirectional persistent connection to enable both server and client side communications with low latency. Abstraction over TCP channels.\n\n   - lifecycle starts as a normal HTTP protocal and can be upgraded via well defined handshake to web socket connection\n   - Connection initiated by client and server can send updates to clients\n   - Uses same 80 && 443 as HTTP connections.\n\n   wss:// /tickets\n\n   - apis in terms of messages we are sending. WS are bit unique compared to SSE - connection can be open for hours.\n   - SSE periodically reconnects to the server. Opening channel and for 30 seconds and closing it and forcing user to reconnect. ?\n   - Good idea to terminate them sometime, so that driver/rider etc are messages are sent to them to message the state handled properly.\n   - Middleware infra - layer for load balancer for websockets. l7 supports ws. WS is a bare TCP connections as such we cannot use L7 functionalities.\n\n2. Naive Polling: Periodically ask server if it has any new messages available.\n\n   - costly\n   - consumes precious resources to answer question that offers no as an answer most of the time.\n\n3. Long Polling: Holds connection open until there are actually data is available or timeout threshold has been reached.\n\n   - sender & reciever may not connect to same server all the time. HTTP based servers are usually stateless\n   - We maintain active connections even when the users aren't active.\n\n   #### Cons:\n\n   - Servers endup processing request for long amount of time, this is state, when we design services, we need to avoid state.\n   - introduces extra latency when transferring data.\n\n4. SSE - Server Sent Events:\n\n- Server side - sets headers, send updates.\n- Every time we get an update instead of sending one monolithic response we send event.\n- works out of box with lot of infra.\n- This is often times best time available to send high frequency data.\n- SSE can send multiple responses within the same http response. basically simulating events, push notifcations we want to send to our client.\n\n5. WebRTC - esoteric of protocols.\n\n- Establish peer to peer connections between clients.\n- Signaling server - tells about publicily addressableport.\n- stun (find publically addressable address ports), turn server(fallback for clients that can't connect to one another).\n\n## Options: Techniques to simulate updates on a server:\n\nPolling repeatedly for updates. DB, it introduces the latency. DB to stores objects so server can query them. It's pretty uncommon. Real Time updates\n\n1. Consistent Hashsing - to assign users to a server, we always know where users are connected. \n2. Pub/Sub - right user message at right time. "},{title:"ZooKeeper",id:"zoo-keeper",markdown:"Zooper handles service discovery, configuration sharing/management, failure detection, leader election, distributed consensus/locks.\n\nWe should handle network delays, partial failures and still maintain consistency to handle above problems.\n\nZookeeper is a synchronized metadata fileSystem - each node connected will have same view of the data. This consistent view across all participating servers is what makes zookeeper powerful for coordination tasks.\n\n\n1. ZooKeeper is a distributed coordination service designed to help manage configuration, naming, synchronization, and group services for distributed applications. It's not meant for bulk data storage but rather for coordinating distributed systems through small metadata operations.\n2. ZNodes are the fundamental data units in ZooKeeper's tree-like namespace. Each ZNode can store small amounts of data (typically under 1MB) along with metadata, and they're organized in a hierarchical structure similar to a file system but where each 'folder' can also contain data.\n3. Ephemeral ZNodes are tied to client sessions and provide automatic cleanup when clients fail. This is crucial for detecting server failures and maintaining accurate service discovery information without manual intervention.\n4. Sequential ZNodes automatically append a monotonically increasing counter, making them perfect for ordered operations like message queues. The sequence numbers ensure messages are processed in the correct order across distributed systems.\n5. ZooKeeper is optimized for small coordination data, not bulk storage. ZNodes are limited to 1MB and the entire dataset is kept in memory for fast access. This design supports high-performance coordination but limits data volume.\n6. ZooKeeper watches are one-time notifications. After a watch fires, it must be re-registered to receive future notifications. This design prevents overwhelming clients with notifications and gives them control over when to re-establish watches.\n7. The leader is responsible for processing all write requests and coordinating updates across the ensemble using the ZAB protocol. This centralized approach ensures consistency and ordering of all state changes.\n8. This design optimizes for read-heavy workloads. Reads are served locally by any server for high throughput, while writes go through the leader to ensure consistency and proper ordering through the ZAB protocol.\n9. The node with the lowest sequence number becomes the leader. This ensures a deterministic election process and natural failover - when the current leader fails, the node with the next lowest sequence number automatically becomes the new leader.\n10. ZooKeeper locks are not designed for high-frequency scenarios. Each lock operation involves creating ZNodes and coordinating across the ensemble, making them better suited for longer-held locks. For high-frequency locking, Redis-based locks are more appropriate.\n11. Each user's connection creates an ephemeral ZNode storing their server location. Other servers can read this ZNode to route messages correctly. When users disconnect, their ephemeral nodes automatically disappear, keeping the routing information current.\n12. ZAB is ZooKeeper's consensus protocol that ensures all servers agree on the state of the system. It handles leader election and atomic broadcast of updates, maintaining consistency even when servers fail or network issues occur.\n13. ZooKeeper provides sequential consistency, meaning updates from a client are applied in the order they were sent. If a client updates node A then node B, all servers will see the update to A before the update to B.\n14. ZooKeeper prioritizes consistency over availability. Without a majority quorum, it stops processing writes to prevent split-brain scenarios where different partitions might make conflicting decisions. This ensures data consistency when the partition is resolved.\n15. Session timeout configuration is critical. Too short timeouts cause false failures during brief network hiccups, while too long timeouts delay detection of actual failures. The typical range is 10-30 seconds to balance responsiveness with stability.\n16. etcd is the coordination service that powers Kubernetes and is popular in cloud-native environments. Like ZooKeeper, it provides distributed key-value storage with strong consistency, but offers modern HTTP/JSON and gRPC APIs that integrate well with cloud-native tooling.\n\n\n\n\n\n\n \n\n\n### Data Model based on ZNodes:\n\n1. organizes data like a tree/hierarichal namespace.\n2. Nodes in tree here are called ZNodes.\n3. Znodes stores upto 1MB of data and metadata and can hold coordination data, not data like images or large documents.\n4. Data stored in znodes is large but the number of znodes itself are thousands.\n\n3 types of Znodes:\n\n    1. Persistent ZNodes: These nodes exists until explicitly deleted. Used for max message size or rate limit parameters.\n    2. Ephemeral ZNodes: Automatically deleted when the session ends, tracking server is alive and which users are online.\n    3. Sequential ZNodes: Automatically appended monotonically increasing counter to their name. Can be used for distributed locks or ordering messages.\n\n### Server roles within a Zookeeper ensemble\n\n1. Zookeeper runs on group of servers called ensemble. A typical production deployment consists of 3,5,or 7 servers.\n2. Server takes on different roles, leader - one server elected responsible for all update requests. When new server is register this write req goes to leader.\n3. Followers, the rest of servers follow leader and serve read requests.\n\n### Watch mechanism that enables real time notifications\n\n1. Solves our chat app's notification problem. Watches allows servers to be notified when a ZNode changes. Eliminates need for constant polling or complex server to server communication.\n\n\n"},{title:"DataModeling",id:"data-modeling",markdown:"Data Modeling - structuring business requirements in form of tables with relationships.\n\n### Questions\n\n1. Data Model for something - app that we use on day-to-day basis. Design DM for uber.\n2. Product sense - Flavor of prod, metrics, create and calculate metrics usign those metrics. Finally build datamodel calculate those metrics. Opening a second hand book store,\n   a. what metrics to find if my book store is running well. - DAUs - MAUs - Duration spent on app - Number of visitors.\n   b. fact_orders, fact_sessions etc.\n3. SQL queries on top of those.\n\nKimball's Modeling??\n\n1. Fact and Dimension tables. - Fact: Events, trasactions, number to measure, count, aggregate. - Dimension: context to fact tables, who, what, when, where.\n2. DIM_PRODUCT, DIM_CUSTOMER, DIM_STORE, DIM_DATE.\n3. Central fact table, 4 dimension table to provide context to the fact table.\n\n- Periodic snapshot fact table\n- factless fact table\n- Accumulating snapshot fact table\n\n- Star tables and Snowflake schema\n- Central fact table has foriegn keys to all fact tbale to be queried and joined.\n- Star is same as snowflake, it's just snowflake is further broken down. dim_product [], dim_category[]\n\n- Relationship among tables:\n- 1:1 , 1: N, N:1\n\n-Slowly changes dimensions, if address changes yoy, how to store in dim customer table? - New address and override it on top of old address. - Keep history have both old and new address. Add a new row with status yes/no or have a start and end period - create a column to hold the new address and we store the new address in that column.\n\n### Approach to final design\n\n1. Identify business process. Ride share - rider, verification, tracker etc\n2. Identify events and entities that could be associated. Payments, reviews, etc. Driver, vehicle entities are dimension table.\n3. List out attributes of the table, driverId, RiderId, requestedAds, pickedup ads etc. Don't worry missing attribtues we will need them for other queries.\n4. Relevant questions, gather requirements to tables.\n\nDesign and deliver data solutions from scratch:\n\n1. Data modeling for airbnb.\n\npurpose of data, is it more transactional or analytical? Data warehousing.\n\nSchema - star schema - denormalized data, it can add redundant cols to tables. Reading is faster.\n\nAnalytics is for reading here. We can use star but storage is concern.\n\nSnow flake which uses normalized data - reduces duplication but is less optimal for reads but good for storage involves more with joins and all.\n\n2. Any directions on metrics? Primarily looking at two metrics customer obession/engagement to improve the experience. business profitability.\n"}],m=e=>{let{...n}=e;return Object(d.jsx)(o.b,{...n})};var p=()=>Object(d.jsx)(l.a,{title:"System Design",description:"This is a system design page",fullPage:!0,children:Object(d.jsxs)("article",{className:"post markdown",id:"system-design",children:[Object(d.jsx)("header",{children:Object(d.jsx)("div",{className:"title",children:Object(d.jsx)("h2",{"data-testid":"heading",children:"System Design Topics"})})}),Object(d.jsx)("ul",{children:u.map((e=>{let{title:n,id:t}=e;return Object(d.jsx)("li",{children:Object(d.jsx)("a",{href:`#${t}`,children:n})},t)}))}),Object(d.jsx)("br",{}),u.map((e=>{let{title:n,id:t,markdown:a}=e;return Object(d.jsxs)("section",{children:[Object(d.jsx)("h2",{id:t,style:{color:"green"},children:n}),Object(d.jsx)(h.a,{source:a,renderers:{Link:m},escapeHtml:!1}),Object(d.jsx)("hr",{})]},t)}))]})});const{PUBLIC_URL:g}=Object({NODE_ENV:"production",PUBLIC_URL:"/personal-site",WDS_SOCKET_HOST:void 0,WDS_SOCKET_PATH:void 0,WDS_SOCKET_PORT:void 0,FAST_REFRESH:!0,REACT_APP_GA_TRACKING_ID:"UA-68649021-1"}),b=Object(a.lazy)((()=>t.e(6).then(t.bind(null,162)))),f=Object(a.lazy)((()=>t.e(4).then(t.bind(null,166)))),w=Object(a.lazy)((()=>t.e(5).then(t.bind(null,167)))),y=Object(a.lazy)((()=>t.e(8).then(t.bind(null,165)))),v=Object(a.lazy)((()=>t.e(7).then(t.bind(null,163))));var k=()=>Object(d.jsx)(o.a,{basename:g,children:Object(d.jsx)(a.Suspense,{fallback:Object(d.jsx)(l.a,{}),children:Object(d.jsxs)(r.c,{children:[Object(d.jsx)(r.a,{exact:!0,path:"/",component:f}),Object(d.jsx)(r.a,{path:"/about",component:b}),Object(d.jsx)(r.a,{path:"/projects",component:w}),Object(d.jsx)(r.a,{path:"/resume",component:y}),Object(d.jsx)(r.a,{path:"/blog",component:v}),Object(d.jsx)(r.a,{path:"/systemdesign",component:p})]})})});const j=()=>Object(d.jsx)(s.a.StrictMode,{children:Object(d.jsx)(k,{})}),x=document.getElementById("root");x.hasChildNodes()?Object(i.hydrate)(Object(d.jsx)(j,{}),x):Object(i.render)(Object(d.jsx)(j,{}),x)},16:function(e,n,t){"use strict";var a=t(1),s=t(36),i=t(3),o=t(26);const{NODE_ENV:r,REACT_APP_GA_TRACKING_ID:l}=Object({NODE_ENV:"production",PUBLIC_URL:"/personal-site",WDS_SOCKET_HOST:void 0,WDS_SOCKET_PATH:void 0,WDS_SOCKET_PORT:void 0,FAST_REFRESH:!0,REACT_APP_GA_TRACKING_ID:"UA-68649021-1"});"production"===r&&o.a.initialize(l);var d=()=>{const{pathname:e}=Object(i.f)();return Object(a.useEffect)((()=>{"production"===r&&(o.a.set({page:e}),o.a.pageview(e))}),[e]),null},c=t(7);var h=[{index:!0,label:"Akhila C'Kolla",path:"/"},{label:"About",path:"/about"},{label:"Resume",path:"/resume"},{label:"Projects",path:"/projects"},{label:"Blog",path:"/blog"}],u=t(0);const m=Object(a.lazy)((()=>t.e(3).then(t.t.bind(null,164,7))));var p=()=>{const[e,n]=Object(a.useState)(!1);return Object(u.jsxs)("div",{className:"hamburger-container",children:[Object(u.jsx)("nav",{className:"main",id:"hambuger-nav",children:Object(u.jsx)("ul",{children:e?Object(u.jsx)("li",{className:"menu close-menu",children:Object(u.jsx)("div",{onClick:()=>n(!e),className:"menu-hover",children:"\u2715"})}):Object(u.jsx)("li",{className:"menu open-menu",children:Object(u.jsx)("div",{onClick:()=>n(!e),className:"menu-hover",children:"\u2630"})})})}),Object(u.jsx)(a.Suspense,{fallback:Object(u.jsx)(u.Fragment,{}),children:Object(u.jsx)(m,{right:!0,isOpen:e,children:Object(u.jsx)("ul",{className:"hamburger-ul",children:h.map((t=>Object(u.jsx)("li",{children:Object(u.jsx)(c.b,{to:t.path,onClick:()=>n(!e),children:Object(u.jsx)("h3",{className:t.index&&"index-li",children:t.label})})},t.label)))})})})]})};var g=()=>Object(u.jsxs)("header",{id:"header",children:[Object(u.jsx)("h1",{className:"index-link",children:h.filter((e=>e.index)).map((e=>Object(u.jsx)(c.b,{to:e.path,children:e.label},e.label)))}),Object(u.jsx)("nav",{className:"links",children:Object(u.jsx)("ul",{children:h.filter((e=>!e.index)).map((e=>Object(u.jsx)("li",{children:Object(u.jsx)(c.b,{to:e.path,children:e.label})},e.label)))})}),Object(u.jsx)(p,{})]}),b=t(58),f=t(59),w=t(60),y=t(61),v=t(62),k=t(63),j=t(64);var x=[{link:"https://github.com/akhikolla",label:"Github",icon:f.faGithub},{link:"https://www.facebook.com/profile.php?id=100080718874806",label:"Facebook",icon:w.faFacebookF},{link:"https://www.instagram.com/akhilachowdarykolla/",label:"Instagram",icon:y.faInstagram},{link:"https://www.linkedin.com/in/akhikolla/",label:"LinkedIn",icon:v.faLinkedinIn},{link:"",label:"Angel List",icon:k.faAngellist},{link:"mailto:akhilakolla5@gmail.com",label:"Email",icon:j.faEnvelope}];var S=()=>Object(u.jsx)("ul",{className:"icons",children:x.map((e=>Object(u.jsx)("li",{children:Object(u.jsx)("a",{href:e.link,children:Object(u.jsx)(b.a,{icon:e.icon})})},e.label)))});const{PUBLIC_URL:O}=Object({NODE_ENV:"production",PUBLIC_URL:"/personal-site",WDS_SOCKET_HOST:void 0,WDS_SOCKET_PATH:void 0,WDS_SOCKET_PORT:void 0,FAST_REFRESH:!0,REACT_APP_GA_TRACKING_ID:"UA-68649021-1"});var T=()=>Object(u.jsxs)("section",{id:"sidebar",children:[Object(u.jsxs)("section",{id:"intro",children:[Object(u.jsx)(c.b,{to:"/",className:"logo",children:Object(u.jsx)("img",{src:`${O}/images/mes.png`,alt:""})}),Object(u.jsxs)("header",{children:[Object(u.jsx)("h2",{children:"Akhila C'Kolla"}),Object(u.jsx)("p",{children:Object(u.jsx)("a",{href:"mailto:akhilakolla5@gmail.com",children:"akhilakolla5@gmail.com"})})]})]}),Object(u.jsxs)("section",{className:"blurb",children:[Object(u.jsx)("h2",{children:"About"}),Object(u.jsxs)("p",{children:[Object(u.jsx)("p",{children:"Hi, I'm Akhila. I like building things."}),Object(u.jsx)("p",{children:"Senior Software Engineer at Amazon Ads, designing reliable, low-latency adtech systems for global publisher integrations."}),Object(u.jsx)("p",{children:"During my downtime, I'm on a tech treasure hunt, scanning through the latest apps and SaaS products on Product Hunt and AppSumo."}),Object(u.jsx)("p",{children:"If I could, I would just code and eat pizza 24 hours a day, but I'm so sad that I have to sleep."}),Object(u.jsxs)("p",{children:["I am a ",Object(u.jsx)("a",{href:"https://nau.edu/",children:"NAU"})," graduate, SVECW alumna, and a graduate research assistant in the",Object(u.jsx)("a",{href:"http://ml.nau.edu/",children:"Machine Learning Research Lab at NAU"}),". Before NAU, I was at ",Object(u.jsx)("a",{href:"https://www.infosys.com/",children:"Infosys"})," ","and",Object(u.jsx)("a",{href:"http://www.apita.ap.gov.in/",children:"APITA"}),"."]})]}),Object(u.jsx)("ul",{className:"actions",children:Object(u.jsx)("li",{children:window.location.pathname.includes("/resume")?Object(u.jsx)(c.b,{to:"/about",className:"button",children:"About Me"}):Object(u.jsx)(c.b,{to:"/resume",className:"button",children:"Learn More"})})})]}),Object(u.jsxs)("section",{id:"footer",children:[Object(u.jsx)(S,{}),Object(u.jsxs)("p",{className:"copyright",children:["\xa9 Akhila C'Kolla",Object(u.jsx)(c.b,{to:"/",children:"akhikolla.com"}),"."]})]})]});var A=()=>{const{pathname:e}=Object(i.f)();return Object(a.useEffect)((()=>{window.scrollTo(0,0)}),[e]),null};const C=e=>Object(u.jsxs)(s.b,{children:[Object(u.jsx)(d,{}),Object(u.jsx)(A,{}),Object(u.jsxs)(s.a,{titleTemplate:"%s | Akhila C'Kolla",defaultTitle:"Akhila C'Kolla",defer:!1,children:[e.title&&Object(u.jsx)("title",{children:e.title}),Object(u.jsx)("meta",{name:"description",content:e.description})]}),Object(u.jsxs)("div",{id:"wrapper",children:[Object(u.jsx)(g,{}),Object(u.jsx)("div",{id:"main",children:e.children}),e.fullPage?null:Object(u.jsx)(T,{})]})]});C.defaultProps={children:null,fullPage:!1,title:null,description:"Akhila Kolla's personal website."};n.a=C},80:function(e,n,t){}},[[154,1,2]]]);
//# sourceMappingURL=main.4a6a51f1.chunk.js.map