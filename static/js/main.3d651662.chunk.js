(this["webpackJsonppersonal-site"]=this["webpackJsonppersonal-site"]||[]).push([[0],{154:function(e,t,n){"use strict";n.r(t);var a=n(1),s=n.n(a),i=n(35),o=n(7),r=n(3),l=n(16),c=(n(80),n(0));var d=n(39),h=n.n(d);const u=[{title:"Top K Problems",id:"topk",markdown:"Design a service which allows us to query the topK most viewed videos in a given time period. The time periods are fixed and might be minute, hour, day, month, or all-time.\n\n<p> </p>\n\n## Functional Requirements (FR)\n\n- Query Top K Most Viewed Videos - Support querying top-K videos (up to 1000)\n- Accept a time period as input\n\n## Non-Functional Requirements (NFR)\n\n- Consistency - Staleness requirement: videos should appear in top-K within 60 seconds\n- Performance - Read latency should be between 10ms to 100ms\n- Scalability - Must handle massive reads and writes\n- Accuracy - Approximations like Bloom Filter, Count-Min Sketch, or PNN are not allowed\n\n## Core Entities\n\nView, Video, Window (min/hour/Day/All-Time)\n\n## API\n\nview comes in we can connect to a stream, api call/kafka topic? represents this?\n\nInput - {videoId}\n\nAPI to get views - GET /views/video?k={k}&window={window} -> [{videoId, views}] - sortedlist\n\n- no, need for video metadata and assume we have a downstream aggregater that can read from me and then augument this videoId with the video names or client can make separate calls to get info.\n\n## HLD\n\nOptimal sol from scalibility and work backwards to system that actual functions.\n\nTake working solution and make it optimal. before NFRs. Don't take all FR together.\n\nshould we solve all time first and then get the window split check?\n\n- We are talking about avaiability -- obvious solution is to replicate the service. If we replicate the service we are bound to enable some kind of the load balancer or an api gateway if we intend deal with or to enable the ratelimiting or authentication and stuff.\n\n- if service fails, LB takes it out and client still have availability.\n- this is a stateful service, if we want to bring back that failed service we have a problem. we should try to manage state within our service such that it doesn't spread across the entire design. TopK is stateful but we need to figure out how to deal it.\n\n- if we lost one instances, we boot it back up and re read those messages from kafka if kafka has retention enabled and mesages and then repopulate the counts. We have to start from scratch. If service is already at bottle neck in terms of views it's ingesting we will have hard time catching up. let's say out system is already 80% utilized on steady state and where it doesn't actually need to catch up and only 20% available to work through backlog. I functionally working with quarter of second it will take more time to catch up.\n\nMost services don't have bunch of excess capacity to rework those jobs, in this case we probably want to do is enable checkpointing.\n\n- write out our counts,heap and ids of last view that we processed that we have to some kind of blob S3/Azure storage. when our service starts up we read from checkpoint, we restore all of the state and then we resume reading from stream from that latest time point, we can do checkpoint in hourly, minutely basis etc but this helps with recover, scale. We need service to add new service we can catch up quickly and check points aren't far behind. For keep consistency.\n\n- How to handle write volumes from kafka topic, we are gonna have multiple shards. How are shards organized. We can assigned that video to one topK service.We can assign that one video to one topK service. Easy way is to take module of number of shards, and videoIds are distributed in semi random fashion. Challenge is we would have to aggregate them across read side.\n\n- TopK service will need to query from each shard and aggregate it. We are going to have top 1000 views from each heap and be able to to accept an arbitary, we can guarantee that global 1000 is gonna be from one of these heap, all we need to do is merge sorted lists and functionally we need to iterate over each item only once. This solution provides way to shards out right and produce a value.\n\n- View stream is partitioned in the same way counters were. How can we make it more elastic? If we need to able to add more shards, we need Zookeeper, where we can keep track of no of shards out there, along side the range of avaiable videos in each shard. If we add a new shards we might need to remove videos from one shard and have to redistribute them, we use checkpoints and retention on kafka streams to pull this off.\n\n- Asynchronous replication with eventual consistency and local reads provides the best balance for streaming systems with strict latency requirements. It allows each node to serve reads immediately without waiting for cross-node synchronization, while still maintaining data durability through asynchronous replication. This approach minimizes latency while ensuring high availability, which is crucial for real-time analytics where slight data inconsistencies are often acceptable in exchange for continuous operation.\n\n- While precomputation typically trades storage for query latency, it doesn't 'always' reduce latency. Precomputed results may become stale, require complex invalidation, or create hotspots. The effectiveness depends on query patterns, update frequency, and cache hit rates.\n\n- When scaling stateful stream processors horizontally, the main challenge is redistributing accumulated state (like counters, heaps, or time windows) across new nodes. Unlike stateless processors that can instantly utilize new capacity, stateful systems must migrate existing state data, which can be expensive and temporarily impact processing during rebalancing. This is why techniques like consistent hashing and incremental state migration are critical for stateful distributed systems.\n\n## Deep Dive\n\n- Of all time counts is solved, how do we handle time windows? a considerable amount of time is needed.\n\n### Approaches:\n\n1. Producing arbitarary aggregation based on time:\n"},{title:"Rate Limiter Design",id:"rate-limiter",markdown:"## Rate Limiter"},{title:"Kafka",id:"kafka",markdown:"## Kafka notes:\n\n* Apache Kafka is an open-source distributed event streaming platform that can be used as either a message queue or stream processing system, designed for high performance, scalability, and durability.\n* Topics are logical groupings of partitions used for publishing and subscribing to data. Partitions are the physical storage units that allow parallel processing, and they are distributed across multiple brokers (servers) in the cluster.\n* Kafka uses a partitioning algorithm that hashes the message key to assign the message to a specific partition. This ensures that messages with the same key always go to the same partition, preserving order at the partition level.\n* Kafka partitions are append-only, immutable logs where messages cannot be altered or deleted once written.Each partition functions as an append-only log file where messages are sequentially added to the end. This immutability is central to Kafka's performance, reliability, and simplifies replication and recovery processes.\n* Consumer groups ensure that each message is processed by exactly one consumer within the group, enabling load balancing and parallel processing while preventing duplicate message processing.\n* Kafka uses a leader-follower replication model where each partition has a leader replica handling reads/writes and multiple follower replicas that passively replicate data, ensuring durability and availability even if brokers fail. This is how it handles fault tolerance for message durability. \n* Kafka uses a pull-based model where consumers actively poll brokers for new messages. This design choice allows consumers to control their consumption rate, prevents overwhelming slow consumers, and enables efficient batching.\n* While message size can be configured, it's recommended to keep messages under 1MB to ensure optimal performance through reduced memory pressure and better network utilization.\n* A hot partition occurs when one partition receives much more traffic than others (e.g., a popular ad getting many clicks), potentially overwhelming that partition's broker while leaving other brokers underutilized.\n* Kafka cannot automatically redistribute hot partitions. Solutions include random partitioning, salting keys with random values, using compound keys, or implementing back pressure - but not automatic redistribution.\n* Setting acks=all ensures that a message is acknowledged only when all in-sync replicas have received it, providing maximum durability guarantees at the cost of slightly higher latency.\n* Consumers commit their offsets to Kafka after processing messages. When a consumer restarts, it reads its last committed offset and resumes from there, ensuring no messages are missed or duplicated.\n* Kafka is designed as a high-performance streaming platform rather than a traditional message queue. Consumer retry patterns must be implemented using separate retry topics and dead letter queues rather than built-in mechanisms.\n* Batching messages allows producers to send multiple messages in a single request, significantly reducing network overhead and improving throughput. This can be configured with maxSize and maxTime settings.\n* Kafka topics have configurable retention policies controlled by retention.ms and retention.bytes settings. The default is to retain messages for 7 days (168 hours) with retention.bytes set to -1 (no size limit). Size limits can be configured but are not set by default.\n\n\n\n\n\n\n\n\n\n\n\n\n\n"},{title:"Real Time Updates",id:"real-time-updates",markdown:"# Examples that need real time updates\n\nGoogle docs work when multiple ppl are making edits, drivers get notifications ride is ready, new comments pop up during live stream.\n\n### Things to Remember:\n\n    1. Polling should be default option. It's stateless and work with many infras\n    2. Always ask interview how real time does this needs to be? pretty real time or fairly fast - using polling.\n\nTypical http follows a request response model, it follows a 3 way handshake and 4 way teardown, burning lot of network overhead for nothing, standard http falls short for real time. This isn't conducive to real time updates.\n\n## Options: Techniques to simulate a server initiated connection\n\n1. WebSockets: Bidirectional persistent connection to enable both server and client side communications with low latency. Abstraction over TCP channels.\n\n   - lifecycle starts as a normal HTTP protocal and can be upgraded via well defined handshake to web socket connection\n   - Connection initiated by client and server can send updates to clients\n   - Uses same 80 && 443 as HTTP connections.\n\n   wss:// /tickets\n\n   - apis in terms of messages we are sending. WS are bit unique compared to SSE - connection can be open for hours.\n   - SSE periodically reconnects to the server. Opening channel and for 30 seconds and closing it and forcing user to reconnect. ?\n   - Good idea to terminate them sometime, so that driver/rider etc are messages are sent to them to message the state handled properly.\n   - Middleware infra - layer for load balancer for websockets. l7 supports ws. WS is a bare TCP connections as such we cannot use L7 functionalities.\n\n2. Naive Polling: Periodically ask server if it has any new messages available.\n\n   - costly\n   - consumes precious resources to answer question that offers no as an answer most of the time.\n\n3. Long Polling: Holds connection open until there are actually data is available or timeout threshold has been reached.\n\n   - sender & reciever may not connect to same server all the time. HTTP based servers are usually stateless\n   - We maintain active connections even when the users aren't active.\n\n   #### Cons:\n\n   - Servers endup processing request for long amount of time, this is state, when we design services, we need to avoid state.\n   - introduces extra latency when transferring data.\n\n4. SSE - Server Sent Events:\n\n- Server side - sets headers, send updates.\n- Every time we get an update instead of sending one monolithic response we send event.\n- works out of box with lot of infra.\n- This is often times best time available to send high frequency data.\n- SSE can send multiple responses within the same http response. basically simulating events, push notifcations we want to send to our client.\n\n5. WebRTC - esoteric of protocols.\n\n- Establish peer to peer connections between clients.\n- Signaling server - tells about publicily addressableport.\n- stun (find publically addressable address ports), turn server(fallback for clients that can't connect to one another).\n\n## Options: Techniques to simulate updates on a server:\n\nPolling repeatedly for updates. DB, it introduces the latency. DB to stores objects so server can query them. It's pretty uncommon. Real Time updates\n\n1. Consistent Hashsing - to assign users to a server, we always know where users are connected. \n2. Pub/Sub - right user message at right time. "},{title:"ZooKeeper",id:"zoo-keeper",markdown:"Zooper handles service discovery, configuration sharing/management, failure detection, leader election, distributed consensus/locks.\n\nWe should handle network delays, partial failures and still maintain consistency to handle above problems.\n\nZookeeper is a synchronized metadata fileSystem - each node connected will have same view of the data. This consistent view across all participating servers is what makes zookeeper powerful for coordination tasks.\n\n\n1. ZooKeeper is a distributed coordination service designed to help manage configuration, naming, synchronization, and group services for distributed applications. It's not meant for bulk data storage but rather for coordinating distributed systems through small metadata operations.\n2. ZNodes are the fundamental data units in ZooKeeper's tree-like namespace. Each ZNode can store small amounts of data (typically under 1MB) along with metadata, and they're organized in a hierarchical structure similar to a file system but where each 'folder' can also contain data.\n3. Ephemeral ZNodes are tied to client sessions and provide automatic cleanup when clients fail. This is crucial for detecting server failures and maintaining accurate service discovery information without manual intervention.\n4. Sequential ZNodes automatically append a monotonically increasing counter, making them perfect for ordered operations like message queues. The sequence numbers ensure messages are processed in the correct order across distributed systems.\n5. ZooKeeper is optimized for small coordination data, not bulk storage. ZNodes are limited to 1MB and the entire dataset is kept in memory for fast access. This design supports high-performance coordination but limits data volume.\n6. ZooKeeper watches are one-time notifications. After a watch fires, it must be re-registered to receive future notifications. This design prevents overwhelming clients with notifications and gives them control over when to re-establish watches.\n7. The leader is responsible for processing all write requests and coordinating updates across the ensemble using the ZAB protocol. This centralized approach ensures consistency and ordering of all state changes.\n8. This design optimizes for read-heavy workloads. Reads are served locally by any server for high throughput, while writes go through the leader to ensure consistency and proper ordering through the ZAB protocol.\n9. The node with the lowest sequence number becomes the leader. This ensures a deterministic election process and natural failover - when the current leader fails, the node with the next lowest sequence number automatically becomes the new leader.\n10. ZooKeeper locks are not designed for high-frequency scenarios. Each lock operation involves creating ZNodes and coordinating across the ensemble, making them better suited for longer-held locks. For high-frequency locking, Redis-based locks are more appropriate.\n11. Each user's connection creates an ephemeral ZNode storing their server location. Other servers can read this ZNode to route messages correctly. When users disconnect, their ephemeral nodes automatically disappear, keeping the routing information current.\n12. ZAB is ZooKeeper's consensus protocol that ensures all servers agree on the state of the system. It handles leader election and atomic broadcast of updates, maintaining consistency even when servers fail or network issues occur.\n13. ZooKeeper provides sequential consistency, meaning updates from a client are applied in the order they were sent. If a client updates node A then node B, all servers will see the update to A before the update to B.\n14. ZooKeeper prioritizes consistency over availability. Without a majority quorum, it stops processing writes to prevent split-brain scenarios where different partitions might make conflicting decisions. This ensures data consistency when the partition is resolved.\n15. Session timeout configuration is critical. Too short timeouts cause false failures during brief network hiccups, while too long timeouts delay detection of actual failures. The typical range is 10-30 seconds to balance responsiveness with stability.\n16. etcd is the coordination service that powers Kubernetes and is popular in cloud-native environments. Like ZooKeeper, it provides distributed key-value storage with strong consistency, but offers modern HTTP/JSON and gRPC APIs that integrate well with cloud-native tooling.\n\n\n\n\n\n\n \n\n\n### Data Model based on ZNodes:\n\n1. organizes data like a tree/hierarichal namespace.\n2. Nodes in tree here are called ZNodes.\n3. Znodes stores upto 1MB of data and metadata and can hold coordination data, not data like images or large documents.\n4. Data stored in znodes is large but the number of znodes itself are thousands.\n\n3 types of Znodes:\n\n    1. Persistent ZNodes: These nodes exists until explicitly deleted. Used for max message size or rate limit parameters.\n    2. Ephemeral ZNodes: Automatically deleted when the session ends, tracking server is alive and which users are online.\n    3. Sequential ZNodes: Automatically appended monotonically increasing counter to their name. Can be used for distributed locks or ordering messages.\n\n### Server roles within a Zookeeper ensemble\n\n1. Zookeeper runs on group of servers called ensemble. A typical production deployment consists of 3,5,or 7 servers.\n2. Server takes on different roles, leader - one server elected responsible for all update requests. When new server is register this write req goes to leader.\n3. Followers, the rest of servers follow leader and serve read requests.\n\n### Watch mechanism that enables real time notifications\n\n1. Solves our chat app's notification problem. Watches allows servers to be notified when a ZNode changes. Eliminates need for constant polling or complex server to server communication.\n\n\n"},{title:"DataModeling",id:"data-modeling",markdown:"Data Modeling - structuring business requirements in form of tables with relationships.\n\n### Questions\n\n1. Data Model for something - app that we use on day-to-day basis. Design DM for uber.\n2. Product sense - Flavor of prod, metrics, create and calculate metrics usign those metrics. Finally build datamodel calculate those metrics. Opening a second hand book store,\n   a. what metrics to find if my book store is running well. - DAUs - MAUs - Duration spent on app - Number of visitors.\n   b. fact_orders, fact_sessions etc.\n3. SQL queries on top of those.\n\nKimball's Modeling??\n\n1. Fact and Dimension tables. - Fact: Events, trasactions, number to measure, count, aggregate. - Dimension: context to fact tables, who, what, when, where.\n2. DIM_PRODUCT, DIM_CUSTOMER, DIM_STORE, DIM_DATE.\n3. Central fact table, 4 dimension table to provide context to the fact table.\n\n- Periodic snapshot fact table\n- factless fact table\n- Accumulating snapshot fact table\n\n- Star tables and Snowflake schema\n- Central fact table has foriegn keys to all fact tbale to be queried and joined.\n- Star is same as snowflake, it's just snowflake is further broken down. dim_product [], dim_category[]\n\n- Relationship among tables:\n- 1:1 , 1: N, N:1\n\n-Slowly changes dimensions, if address changes yoy, how to store in dim customer table? - New address and override it on top of old address. - Keep history have both old and new address. Add a new row with status yes/no or have a start and end period - create a column to hold the new address and we store the new address in that column.\n\n### Approach to final design\n\n1. Identify business process. Ride share - rider, verification, tracker etc\n2. Identify events and entities that could be associated. Payments, reviews, etc. Driver, vehicle entities are dimension table.\n3. List out attributes of the table, driverId, RiderId, requestedAds, pickedup ads etc. Don't worry missing attribtues we will need them for other queries.\n4. Relevant questions, gather requirements to tables.\n\nDesign and deliver data solutions from scratch:\n\n1. Data modeling for airbnb.\n\npurpose of data, is it more transactional or analytical? Data warehousing.\n\nSchema - star schema - denormalized data, it can add redundant cols to tables. Reading is faster.\n\nAnalytics is for reading here. We can use star but storage is concern.\n\nSnow flake which uses normalized data - reduces duplication but is less optimal for reads but good for storage involves more with joins and all.\n\n2. Any directions on metrics? Primarily looking at two metrics customer obession/engagement to improve the experience. business profitability.\n"}],m=e=>{let{...t}=e;return Object(c.jsx)(o.b,{...t})};var p=()=>Object(c.jsx)(l.a,{title:"System Design",description:"This is a system design page",fullPage:!0,children:Object(c.jsxs)("article",{className:"post markdown",id:"system-design",children:[Object(c.jsx)("header",{children:Object(c.jsx)("div",{className:"title",children:Object(c.jsx)("h2",{"data-testid":"heading",children:"System Design Topics"})})}),Object(c.jsx)("ul",{children:u.map((e=>{let{title:t,id:n}=e;return Object(c.jsx)("li",{children:Object(c.jsx)("a",{href:`#${n}`,children:t})},n)}))}),Object(c.jsx)("br",{}),u.map((e=>{let{title:t,id:n,markdown:a}=e;return Object(c.jsxs)("section",{children:[Object(c.jsx)("h2",{id:n,style:{color:"green"},children:t}),Object(c.jsx)(h.a,{source:a,renderers:{Link:m},escapeHtml:!1}),Object(c.jsx)("hr",{})]},n)}))]})});const{PUBLIC_URL:b}=Object({NODE_ENV:"production",PUBLIC_URL:"/personal-site",WDS_SOCKET_HOST:void 0,WDS_SOCKET_PATH:void 0,WDS_SOCKET_PORT:void 0,FAST_REFRESH:!0,REACT_APP_GA_TRACKING_ID:"UA-68649021-1"}),g=Object(a.lazy)((()=>n.e(6).then(n.bind(null,162)))),f=Object(a.lazy)((()=>n.e(4).then(n.bind(null,166)))),w=Object(a.lazy)((()=>n.e(5).then(n.bind(null,167)))),v=Object(a.lazy)((()=>n.e(8).then(n.bind(null,165)))),y=Object(a.lazy)((()=>n.e(7).then(n.bind(null,163))));var j=()=>Object(c.jsx)(o.a,{basename:b,children:Object(c.jsx)(a.Suspense,{fallback:Object(c.jsx)(l.a,{}),children:Object(c.jsxs)(r.c,{children:[Object(c.jsx)(r.a,{exact:!0,path:"/",component:f}),Object(c.jsx)(r.a,{path:"/about",component:g}),Object(c.jsx)(r.a,{path:"/projects",component:w}),Object(c.jsx)(r.a,{path:"/resume",component:v}),Object(c.jsx)(r.a,{path:"/blog",component:y}),Object(c.jsx)(r.a,{path:"/systemdesign",component:p})]})})});const k=()=>Object(c.jsx)(s.a.StrictMode,{children:Object(c.jsx)(j,{})}),O=document.getElementById("root");O.hasChildNodes()?Object(i.hydrate)(Object(c.jsx)(k,{}),O):Object(i.render)(Object(c.jsx)(k,{}),O)},16:function(e,t,n){"use strict";var a=n(1),s=n(36),i=n(3),o=n(26);const{NODE_ENV:r,REACT_APP_GA_TRACKING_ID:l}=Object({NODE_ENV:"production",PUBLIC_URL:"/personal-site",WDS_SOCKET_HOST:void 0,WDS_SOCKET_PATH:void 0,WDS_SOCKET_PORT:void 0,FAST_REFRESH:!0,REACT_APP_GA_TRACKING_ID:"UA-68649021-1"});"production"===r&&o.a.initialize(l);var c=()=>{const{pathname:e}=Object(i.f)();return Object(a.useEffect)((()=>{"production"===r&&(o.a.set({page:e}),o.a.pageview(e))}),[e]),null},d=n(7);var h=[{index:!0,label:"Akhila C'Kolla",path:"/"},{label:"About",path:"/about"},{label:"Resume",path:"/resume"},{label:"Projects",path:"/projects"},{label:"Blog",path:"/blog"}],u=n(0);const m=Object(a.lazy)((()=>n.e(3).then(n.t.bind(null,164,7))));var p=()=>{const[e,t]=Object(a.useState)(!1);return Object(u.jsxs)("div",{className:"hamburger-container",children:[Object(u.jsx)("nav",{className:"main",id:"hambuger-nav",children:Object(u.jsx)("ul",{children:e?Object(u.jsx)("li",{className:"menu close-menu",children:Object(u.jsx)("div",{onClick:()=>t(!e),className:"menu-hover",children:"\u2715"})}):Object(u.jsx)("li",{className:"menu open-menu",children:Object(u.jsx)("div",{onClick:()=>t(!e),className:"menu-hover",children:"\u2630"})})})}),Object(u.jsx)(a.Suspense,{fallback:Object(u.jsx)(u.Fragment,{}),children:Object(u.jsx)(m,{right:!0,isOpen:e,children:Object(u.jsx)("ul",{className:"hamburger-ul",children:h.map((n=>Object(u.jsx)("li",{children:Object(u.jsx)(d.b,{to:n.path,onClick:()=>t(!e),children:Object(u.jsx)("h3",{className:n.index&&"index-li",children:n.label})})},n.label)))})})})]})};var b=()=>Object(u.jsxs)("header",{id:"header",children:[Object(u.jsx)("h1",{className:"index-link",children:h.filter((e=>e.index)).map((e=>Object(u.jsx)(d.b,{to:e.path,children:e.label},e.label)))}),Object(u.jsx)("nav",{className:"links",children:Object(u.jsx)("ul",{children:h.filter((e=>!e.index)).map((e=>Object(u.jsx)("li",{children:Object(u.jsx)(d.b,{to:e.path,children:e.label})},e.label)))})}),Object(u.jsx)(p,{})]}),g=n(58),f=n(59),w=n(60),v=n(61),y=n(62),j=n(63),k=n(64);var O=[{link:"https://github.com/akhikolla",label:"Github",icon:f.faGithub},{link:"https://www.facebook.com/profile.php?id=100080718874806",label:"Facebook",icon:w.faFacebookF},{link:"https://www.instagram.com/akhilachowdarykolla/",label:"Instagram",icon:v.faInstagram},{link:"https://www.linkedin.com/in/akhikolla/",label:"LinkedIn",icon:y.faLinkedinIn},{link:"",label:"Angel List",icon:j.faAngellist},{link:"mailto:akhilakolla5@gmail.com",label:"Email",icon:k.faEnvelope}];var x=()=>Object(u.jsx)("ul",{className:"icons",children:O.map((e=>Object(u.jsx)("li",{children:Object(u.jsx)("a",{href:e.link,children:Object(u.jsx)(g.a,{icon:e.icon})})},e.label)))});const{PUBLIC_URL:T}=Object({NODE_ENV:"production",PUBLIC_URL:"/personal-site",WDS_SOCKET_HOST:void 0,WDS_SOCKET_PATH:void 0,WDS_SOCKET_PORT:void 0,FAST_REFRESH:!0,REACT_APP_GA_TRACKING_ID:"UA-68649021-1"});var S=()=>Object(u.jsxs)("section",{id:"sidebar",children:[Object(u.jsxs)("section",{id:"intro",children:[Object(u.jsx)(d.b,{to:"/",className:"logo",children:Object(u.jsx)("img",{src:`${T}/images/mes.png`,alt:""})}),Object(u.jsxs)("header",{children:[Object(u.jsx)("h2",{children:"Akhila C'Kolla"}),Object(u.jsx)("p",{children:Object(u.jsx)("a",{href:"mailto:akhilakolla5@gmail.com",children:"akhilakolla5@gmail.com"})})]})]}),Object(u.jsxs)("section",{className:"blurb",children:[Object(u.jsx)("h2",{children:"About"}),Object(u.jsxs)("p",{children:[Object(u.jsx)("p",{children:"Hi, I'm Akhila. I like building things."}),Object(u.jsx)("p",{children:"Senior Software Engineer at Amazon Ads, designing reliable, low-latency adtech systems for global publisher integrations."}),Object(u.jsx)("p",{children:"During my downtime, I'm on a tech treasure hunt, scanning through the latest apps and SaaS products on Product Hunt and AppSumo."}),Object(u.jsx)("p",{children:"If I could, I would just code and eat pizza 24 hours a day, but I'm so sad that I have to sleep."}),Object(u.jsxs)("p",{children:["I am a ",Object(u.jsx)("a",{href:"https://nau.edu/",children:"NAU"})," graduate, SVECW alumna, and a graduate research assistant in the",Object(u.jsx)("a",{href:"http://ml.nau.edu/",children:"Machine Learning Research Lab at NAU"}),". Before NAU, I was at ",Object(u.jsx)("a",{href:"https://www.infosys.com/",children:"Infosys"})," ","and",Object(u.jsx)("a",{href:"http://www.apita.ap.gov.in/",children:"APITA"}),"."]})]}),Object(u.jsx)("ul",{className:"actions",children:Object(u.jsx)("li",{children:window.location.pathname.includes("/resume")?Object(u.jsx)(d.b,{to:"/about",className:"button",children:"About Me"}):Object(u.jsx)(d.b,{to:"/resume",className:"button",children:"Learn More"})})})]}),Object(u.jsxs)("section",{id:"footer",children:[Object(u.jsx)(x,{}),Object(u.jsxs)("p",{className:"copyright",children:["\xa9 Akhila C'Kolla",Object(u.jsx)(d.b,{to:"/",children:"akhikolla.com"}),"."]})]})]});var A=()=>{const{pathname:e}=Object(i.f)();return Object(a.useEffect)((()=>{window.scrollTo(0,0)}),[e]),null};const N=e=>Object(u.jsxs)(s.b,{children:[Object(u.jsx)(c,{}),Object(u.jsx)(A,{}),Object(u.jsxs)(s.a,{titleTemplate:"%s | Akhila C'Kolla",defaultTitle:"Akhila C'Kolla",defer:!1,children:[e.title&&Object(u.jsx)("title",{children:e.title}),Object(u.jsx)("meta",{name:"description",content:e.description})]}),Object(u.jsxs)("div",{id:"wrapper",children:[Object(u.jsx)(b,{}),Object(u.jsx)("div",{id:"main",children:e.children}),e.fullPage?null:Object(u.jsx)(S,{})]})]});N.defaultProps={children:null,fullPage:!1,title:null,description:"Akhila Kolla's personal website."};t.a=N},80:function(e,t,n){}},[[154,1,2]]]);
//# sourceMappingURL=main.3d651662.chunk.js.map